{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Search\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vqpy/vqpy/blob/main/examples/person_search/demo.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Person search could retrieve the target person from videos across different camera views.\n",
    "\n",
    "In this example we use models in [Fast-ReID](https://github.com/JDAI-CV/fast-reid/) to match the query and gallery person."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install VQPy from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy==1.23.5 cython\n",
    "!pip install 'vqpy @ git+https://github.com/vqpy/vqpy.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "2. Install other dependencies for additional models Fast-ReID [here](https://github.com/JDAI-CV/fast-reid/blob/master/INSTALL.md).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/JDAI-CV/fast-reid/master/docs/requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Download video from [here](https://drive.google.com/file/d/1xkCr6uY-wp0ZdhJfEkhd_7XOc0qNmOcq/view?usp=sharing) and place it in the same directory as this notebook.\n",
    "\n",
    "5. Set paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_path = \"./video/camera.mp4\"\t# path to video\n",
    "query_folder = \"./query/\"           # folder containing query images\n",
    "save_folder = \"./vqpy_outputs\"      # folder to save the query result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the working directory should look like:\n",
    "\n",
    "```text\n",
    ".\n",
    "├── video           # camera video examples\n",
    "│   └── camera.mp4  # video to query on\n",
    "│── query           # query person\n",
    "│   └── query_0.jpg # person images for query\n",
    "└── vqpy            # VQPy repo\n",
    "    └── vqpy        # VQPy library\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Search with VQPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define `VObj` type for person"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in person search, we create a `Person` VObj, which is implemented with two additional properties.\n",
    "\n",
    "\n",
    "- To store the person features, we create a `feature` property, using a pretrained model to extract the image features. The pretrained models are optional by following the Fast-ReID instruction [here](https://github.com/JDAI-CV/fast-reid/blob/master/MODEL_ZOO.md). In our example, we use the [BoT](http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf) backbone pretrained on [MSMT17](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Person_Transfer_GAN_CVPR_2018_paper.pdf) datasets. Additionally, to precisely evaluate the `feature` property, we decorate the `feature` property with `@stateful(30)`, where we can store the person features of last 30 frames.\n",
    "\n",
    "- Since person search always has more than one query object, we create a `candidate` property for `Person` VObj to store the most matched query object IDs, and the corresponding similarity distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import vqpy\n",
    "import sys\n",
    "\n",
    "class Person(vqpy.VObjBase):\n",
    "    required_fields = ['class_id', 'image']\n",
    "\n",
    "    feature_predictor = None\n",
    "    gallery_features = None\n",
    "\n",
    "    @vqpy.property()\n",
    "    @vqpy.stateful(30)\n",
    "    def feature(self):\n",
    "        \"\"\"\n",
    "        extract the feature of person image\n",
    "        :return: feature vector, shape = (N,)\n",
    "        \"\"\"\n",
    "        image = self.getv('image')\n",
    "        if image is None:\n",
    "            return None\n",
    "        return Person.feature_predictor(image).reshape(-1)\n",
    "\n",
    "    @vqpy.property()\n",
    "    def candidate(self):\n",
    "        \"\"\"\n",
    "        retrieve the top-1 similar query object as the searching candidate\n",
    "        :returns:\n",
    "            ids (int): query IDs with most similarity\n",
    "            dist (float): the similarity distance with [0, 1]\n",
    "        \"\"\"\n",
    "        query_features = [self.getv('feature', (-1) * i) for i in range(1, 31)]\n",
    "        gallery_features = self.getv('gallery_features')\n",
    "\n",
    "        # compare the feature distance for different target person\n",
    "        past_ids, past_dist = [], []\n",
    "        for query_feature in query_features:\n",
    "            # iterate features from the last 30 frames\n",
    "            if query_feature is not None:\n",
    "                dist = np.dot(gallery_features, query_feature)  # cosine similarity distance\n",
    "                past_ids.append(np.argmax(dist))  # the most similar IDs\n",
    "                past_dist.append(np.max(dist))  # the most similarity distance\n",
    "\n",
    "        ids = np.argmax(np.bincount(past_ids))  # the most matched IDs\n",
    "        dist = np.mean(past_dist)  # the mean distance over past matching\n",
    "\n",
    "        return ids, dist\n",
    "\n",
    "\n",
    "# load pre-trained models for person feature extracting\n",
    "sys.path.append(\"VQPy/examples/person_search/\")\n",
    "from models import ReIDPredictor\n",
    "feature_predictor = ReIDPredictor(cfg=\"MSMT17/bagtricks_R50.yml\")\n",
    "\n",
    "# extract the feature of query images\n",
    "gallery_features = []\n",
    "for file_name in os.listdir(query_folder):\n",
    "    # extract features for all images from given directory\n",
    "    img_path = os.path.join(query_folder, file_name)\n",
    "    preds = feature_predictor(img_path)\n",
    "    gallery_features.append(preds)\n",
    "\n",
    "gallery_features = np.concatenate(gallery_features, axis=0)\n",
    "\n",
    "Person.feature_predictor = feature_predictor\n",
    "Person.gallery_features = gallery_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Query on `Person` retrieval\n",
    "\n",
    "The `Person` VObj has the `candidate` property to describe the most similar query object IDs and the corresponding score. We pre-defined a threshold `0.97` for candidate score to filter out the matching person in videos.\n",
    "\n",
    "`filter_cons` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cons = {\n",
    "    '__class__': lambda x: x == Person,\n",
    "    'candidate': lambda x: x[1] >= 0.97,  # similar threshold\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For output, we select:\n",
    "\n",
    "- tracker id, selected with `track_id`\n",
    "- candidate id, selected with the `candidate[0]`. Need to be converted to `str` before serializing.\n",
    "- bounding box, in format of coordinates of top-left and bottom-right corner, selected with `tlbr`. Need to be converted to `str` before serializing.\n",
    "\n",
    "`select_cons` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cons = {\n",
    "    'track_id': None,\n",
    "    'candidate': lambda x: str(x[0]),  # convert IDs to string\n",
    "                                       # for JSON serialization\n",
    "    'tlbr': lambda x: str(x),  # convert to string\n",
    "                               # for JSON serialization\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonSearch(vqpy.QueryBase):\n",
    "    \"\"\"The class searching target person from videos\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def setting() -> vqpy.VObjConstraint:\n",
    "        return vqpy.VObjConstraint(\n",
    "            filter_cons=filter_cons,\n",
    "            select_cons=select_cons,\n",
    "            filename='person_search'\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the query\n",
    "\n",
    "With the `Person` VObj and the query defined, we can run the query, with:\n",
    "\n",
    "- `cls_name` is a tuple for mapping numerical outputs of object detector to literal detection class name\n",
    "\n",
    "\tHere we use `COCO_CLASSES` since it includes all the class names of interest in the fall detection query, i.e. `\"person\"`.\n",
    "\n",
    "- dictionary `cls_type` is then used to map detection class name (in str) to VObj types defined\n",
    "\n",
    "\t`{\"person\": Person}` means we wish to map COCO class `person` to VObj type `Person`\n",
    "\n",
    "- `tasks` is a list of queries to run on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqpy.launch(\n",
    "    cls_name=vqpy.COCO_CLASSES,\n",
    "    cls_type={\"person\": Person},\n",
    "    tasks=[PersonSearch()],\n",
    "    video_path=video_path,\n",
    "    save_folder=save_folder,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the query will be in `{save_folder}/{video_name}_{task_name}_{detector_name}.json`, output for this example should be in `./vqpy_outputs/person_search_yolox.json`.\n",
    "\n",
    "One entry is created for each frame that has filter condition satisfied.\n",
    "\n",
    "e.g. The query person images are captured in advance:\n",
    "\n",
    "<img src=\"./demo.assets/query.jpg\">\n",
    "\n",
    "Retrieve the target person on the camera videos:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"frame_id\": 11,\n",
    "  \"data\": [\n",
    "    {\n",
    "       \"track_id\": 2,\n",
    "       \"match\": \"0\",\n",
    "       \"tlbr\": \"[155.7875 232.65001 224.5375 431.75 ]\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Visualize the query person in videos, and the person in red bounding box is the retrieved object:\n",
    "\n",
    "<img src=\"./demo.assets/marked.jpg\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
